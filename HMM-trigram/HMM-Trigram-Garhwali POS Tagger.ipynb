{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import tnt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62580aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path_training=\"POS-tagger-Garhwali-language/Data/Training_Data.txt\"\n",
    "file_train=open(root_path_training,\"r\",encoding=\"UTF-8\")\n",
    "train_=file_train.readlines()\n",
    "print(\"number of training sentences : \",len(train_))\n",
    "\n",
    "\n",
    "root_path_test=\"POS-tagger-Garhwali-language/Data/Test_Data.txt\"\n",
    "file_test=open(root_path_test,\"r\",encoding=\"UTF-8\")\n",
    "test_=file_test.readlines()\n",
    "print(\"number of test sentences : \",len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#----- FUNCTION TO CREATE LIST OF LIST OF TUPLE(WORD,TAG)----------------#\n",
    "###############################################################################\n",
    "def sent_to_labels(data):\n",
    "    content=[]\n",
    "    for sentence in data:\n",
    "        sent=[]\n",
    "        sentence=sentence.strip(\"\\n\")\n",
    "        for wrd_tg in sentence.split(\" \"):\n",
    "            w_t=tuple(wrd_tg.split(\"_\"))\n",
    "            sent.append(w_t)\n",
    "        content.append(sent)\n",
    "    return content    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e657e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#----- FUNCTION TO CREATE LIST OF LIST OF WORD----------------#\n",
    "###############################################################################\n",
    "def validate_sentences(validate_text):\n",
    "    validate=[]\n",
    "    for lines in validate_text:\n",
    "        sent=[]\n",
    "        for tagged in lines.split():\n",
    "            wrd=tagged.split(\"_\")[0]\n",
    "            sent.append(wrd)\n",
    "        validate.append(sent)\n",
    "    return validate    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0898202",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#---- CALLING FUNCITON TO CREATE THE TRAIN DATA LIST [[(W1,T1),(W2,T2)]]\n",
    "############################################################################\n",
    "train_data=sent_to_labels(train_)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#---- CALLING FUNCITON TO CREATE THE VALIDATE DATA LIST [[(W1,T1),(W2,T2)]]\n",
    "############################################################################\n",
    "test_data=sent_to_labels(test_)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "#---- CALLING FUNCITON TO CREATE THE LIST OF SENTENCE TO VALIDATE DATA ON [[\"W1\",\"W2\"....]]\n",
    "############################################################################################\n",
    "\n",
    "test_sent=validate_sentences(test_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the trigram model\n",
    "pos_trigram=tnt.TnT(N=5000)\n",
    "pos_trigram.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "pos_trigram.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf58f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9af6a78",
   "metadata": {},
   "source": [
    "# **Added Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392eb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sent = pos_trigram.tag_sents(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7619d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85a0399f",
   "metadata": {},
   "source": [
    "# **1. Reduplicative Tag** \n",
    "*two consecutive occurrence of the word or the word-word form. Second occurrence will be tagged as RDP*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in tagged_sent:\n",
    "    ret=len(ts)-2\n",
    "    for w in range(ret):\n",
    "        if(ts[w][0]==ts[w+1][0]):\n",
    "            ts[w+1]=list(ts[w])\n",
    "            print(ts[w][0],ts[w+1][0])\n",
    "            ts[w+1][1]='RDP'\n",
    "            ts[w+1]=tuple(ts[w+1])\n",
    "        if(ts[w][0]==ts[w+2][0] and ts[w+1][0]=='-'):\n",
    "            #print(ts[w][0],ts[w+2][0])\n",
    "            ts[w+2]=list(ts[w+2])\n",
    "            ts[w+2][1]='RDP'\n",
    "            ts[w+2]=tuple(ts[w+2])\n",
    "            \n",
    "\n",
    "\n",
    "# ACCURACY - RDP feature added\n",
    "list_manual=[word[1] for sent in tagged_sent for word in sent]\n",
    "list_tagger=[word[1] for sent in test_data for word in sent]\n",
    "matrix=confusion_matrix(list_tagger,list_manual,labels=list(set(list_tagger)))\n",
    "df=pd.DataFrame(matrix,index=list(set(list_tagger)),columns=list(set(list_tagger)))\n",
    "summ=0\n",
    "acuuracy=0\n",
    "for tag in list(set(list_tagger)):\n",
    "    summ+=df[tag][tag]\n",
    "    \n",
    "total = len([word for sent in tagged_sent for word in sent])\n",
    "accuracy=summ/total       \n",
    "\n",
    "print(accuracy)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03549836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f163d8c",
   "metadata": {},
   "source": [
    "# **2. Unknown Tag**\n",
    "\n",
    "*words only tagged as unknown in the training set will be tagged as UNK in the test set also*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words only tagged as UNK in the train data will be tagged as UNK  \n",
    "\n",
    "unknown_wrd=train_data\n",
    "unknown_wrd=[word[0] for sent in train_data for word in sent if(word[1]=='UNK')]\n",
    "allwrd=[word for sent in train_data for word in sent]\n",
    "unk_tags={}\n",
    "\n",
    "#list of unknown words\n",
    "for unk in unknown_wrd:\n",
    "    temp=[]\n",
    "    for word in allwrd:\n",
    "        if(unk==word[0]):\n",
    "            temp.append(word[1])\n",
    "    unk_tags[unk]=list(set(temp))        \n",
    "unknown_wrd=[]\n",
    "for word in unk_tags:\n",
    "    if(unk_tags[word]==['UNK']):\n",
    "        unknown_wrd.append(word)\n",
    "        \n",
    "for sent in tagged_sent:\n",
    "    for word in sent:\n",
    "        if(word[0] in unknown_wrd):\n",
    "            word=list(word)\n",
    "            word[1]=\"UNK\"\n",
    "            word=tuple(word)\n",
    "            \n",
    "            \n",
    "            \n",
    "## ACCURACY CALCULATION\n",
    "list_manual=[word[1] for sent in tagged_sent for word in sent]\n",
    "list_tagger=[word[1] for sent in test_data for word in sent]\n",
    "matrix=confusion_matrix(list_tagger,list_manual,labels=list(set(list_tagger)))\n",
    "df=pd.DataFrame(matrix,index=list(set(list_tagger)),columns=list(set(list_tagger)))\n",
    "summ=0\n",
    "acuuracy=0\n",
    "for tag in list(set(list_tagger)):\n",
    "    summ+=df[tag][tag]\n",
    "\n",
    "total = len([word for sent in tagged_sent for word in sent])\n",
    "accuracy=summ/total   \n",
    "print(accuracy)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376d9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b309b407",
   "metadata": {},
   "source": [
    "# **3. Symbol Tag**\n",
    "*All the special characters except for the numeric characters will be tagged as SYM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c544c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special Characters in the data will be tagged as SYM\n",
    "\n",
    "sym=[word[0] for sent in train_data for word in sent if(word[1] == 'SYM')]\n",
    "sym=list(set(sym))\n",
    "for sent in tagged_sent:\n",
    "    leng=len(sent)-1\n",
    "    for w in range(leng):\n",
    "        if(sent[w][1]=='SYM'):\n",
    "            if not (sent[w][0] in sym):\n",
    "                print(sent)\n",
    "                s= list(\"{} {} {}\".format(sent[w-1][0],sent[w][0],sent[w+1][0]))\n",
    "                tag=tagger.tag([sent[w][0]])\n",
    "                sent[w]=list(sent[w])\n",
    "                sent[w][1]=tag[0][1]\n",
    "                sent[w]=tuple(sent[w])\n",
    "              \n",
    "\n",
    "## ACCURACY CALCULATION\n",
    "list_manual=[word[1] for sent in tagged_sent for word in sent]\n",
    "list_tagger=[word[1] for sent in test_data for word in sent]\n",
    "matrix=confusion_matrix(list_tagger,list_manual,labels=list(set(list_tagger)))\n",
    "df=pd.DataFrame(matrix,index=list(set(list_tagger)),columns=list(set(list_tagger)))\n",
    "summ=0\n",
    "acuuracy=0\n",
    "for tag in list(set(list_tagger)):\n",
    "    summ+=df[tag][tag]\n",
    "    \n",
    "    \n",
    "total = len([word for sent in tagged_sent for word in sent])\n",
    "accuracy=summ/total   \n",
    "\n",
    "print(accuracy)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485570f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aee70911",
   "metadata": {},
   "source": [
    "# **4. Verb Tag**\n",
    "*words containing entire word which tagged as NN and ending with ['ली' , 'लि' , 'नि' , 'नी'  , 'ण' , 'णी' , 'णि ' , ' ु', ' ू' ]   will tagged as VM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_words = [word[0] for sent in train_data for word in sent if(word[1]=='NN')]\n",
    "# all_wrd= [word for sent in train_data for word in sent]\n",
    "# NV_words = []\n",
    "# for word in all_wrd:\n",
    "#     if(word[0] in NN_words):\n",
    "#         if(word[1] == 'NN' and (word[1] == 'VM' or word[1] == 'VAUX')):\n",
    "#             NV_words.append(word[0])\n",
    "\n",
    "            \n",
    "# ending_diacritic=['ली' , 'लि' , 'नि' , 'नी'  , 'ण' , 'णी' , 'णि ' , ' ु', ' ू' ]            \n",
    "# for sent in tagged_sent:\n",
    "#     for word in sent:\n",
    "#         Val=0\n",
    "#         for w in NV_words:\n",
    "#             if(word[0] in w):\n",
    "#                 Val=1\n",
    "            \n",
    "#         if(Val == 1):\n",
    "#             if(word[-2:] in ending_diacritic or word[-1] in ending_diacritic):\n",
    "#                 word=list(word)\n",
    "#                 word[1]='VM'\n",
    "#                 word=tuple(word)\n",
    "\n",
    "\n",
    "# ## ACCURACY CALCULATION\n",
    "# list_manual=[word[1] for sent in tagged_sent for word in sent]\n",
    "# list_tagger=[word[1] for sent in test_data for word in sent]\n",
    "# matrix=confusion_matrix(list_tagger,list_manual,labels=list(set(list_tagger)))\n",
    "# df=pd.DataFrame(matrix,index=list(set(list_tagger)),columns=list(set(list_tagger)))\n",
    "# summ=0\n",
    "# acuuracy=0\n",
    "# for tag in list(set(list_tagger)):\n",
    "#     summ+=df[tag][tag]\n",
    "#     #print(tag,summ)\n",
    "\n",
    "# total = len([word for sent in tagged_sent for word in sent])\n",
    "# accuracy=summ/total   \n",
    "\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46ddd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd27c5af",
   "metadata": {},
   "source": [
    "# **5.Auxiliary Words**\n",
    "*words after Verb will be tagged as Auxiliary Verb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54048533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #list of words that are tagged as VERB or VAUX\n",
    "# v_wrd=train_data\n",
    "# v_wrd=[word[0] for sent in train_data for word in sent if(word[1]=='VERB' or word[1]=='VAUX')]\n",
    "# allwrd=[word for sent in train_data for word in sent]\n",
    "# v_tags={}\n",
    "\n",
    "# #list of verb or vaux words\n",
    "# for v in v_wrd:\n",
    "#     temp=[]\n",
    "#     for word in allwrd:\n",
    "#         if(v==word[0]):\n",
    "#             temp.append(word[1])\n",
    "#     v_tags[v]=list(set(temp))        \n",
    "\n",
    "# vv_words=[]    \n",
    "# for word in v_tags:\n",
    "#     if(len(v_tags[word]) <= 2):\n",
    "#         if(v_tags[word] in ['VAUX','VM']):\n",
    "#             vv_words.append(word)\n",
    "            \n",
    "# for sent in tagged_sent:\n",
    "#     leng=len(sent)-1\n",
    "#     for w in range(leng):\n",
    "#         if(sent[w][1] in ['VM','VAUX'] and sent[w+1][0] in vv_words):\n",
    "#             sent[w+1]=list(sent[w+1])\n",
    "#             sent[w+1][1]='VAUX'\n",
    "#             sent[w+1]=tuple(sent[w+1])\n",
    "\n",
    "\n",
    "\n",
    "# ## ACCURACY CALCULATION\n",
    "# list_manual=[word[1] for sent in tagged_sent for word in sent]\n",
    "# list_tagger=[word[1] for sent in test_data for word in sent]\n",
    "# matrix=confusion_matrix(list_tagger,list_manual,labels=list(set(list_tagger)))\n",
    "# df=pd.DataFrame(matrix,index=list(set(list_tagger)),columns=list(set(list_tagger)))\n",
    "# summ=0\n",
    "# acuuracy=0\n",
    "# for tag in list(set(list_tagger)):\n",
    "#     summ+=df[tag][tag]\n",
    "#     print(tag,summ)\n",
    "\n",
    "\n",
    "# total = len([word for sent in tagged_sent for word in sent])\n",
    "# accuracy=summ/total \n",
    "\n",
    "\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9921ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "481cb424",
   "metadata": {},
   "source": [
    "# **6. Using StanfordNlp Library**\n",
    "*Using Stanfordnlp library to deal with adjective and adverb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9541c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stanfordnlp\n",
    "# nlp = stanfordnlp.Pipeline(processors = 'tokenize,pos' , lang = 'hi' , models_dir= r\"C:\\Users\\Hp\\stanfordnlp_resources\")\n",
    "\n",
    "\n",
    "# jr_words = [word[0] for sent in train_data for word in sent if(word[1] == 'RB' or word[1] == 'JJ')]\n",
    "# jr_words = list(set(jr_words))\n",
    "\n",
    "# acceptable=['JJ','RB']\n",
    "\n",
    "# jr_dict={}\n",
    "# for word in jr_words:\n",
    "#     jr_dict[word]=[]\n",
    "\n",
    "\n",
    "\n",
    "# for sent in train_data:\n",
    "#     leng=len(sent)\n",
    "#     for w in range(leng):\n",
    "#         if(sent[w][0] in jr_words and sent[w][1] in acceptable):\n",
    "#             jr_dict[sent[w][0]].append(sent[w][1])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "# for word in jr_dict:\n",
    "#     jr_dict[word]=list(set(jr_dict[word]))\n",
    "                \n",
    "\n",
    "# #dictionary that contains pos tags and their explanations\n",
    "# pos_dict = {\"ADJ\":\"JJ\",\n",
    "#             \"ADV\":\"RB\"}\n",
    "\n",
    "# #extract parts of speech\n",
    "# def extract_pos(doc):\n",
    "#     for sent in doc.sentences:\n",
    "        \n",
    "#         s=[]\n",
    "#         for wrd in sent.words:\n",
    "#             if wrd.pos in pos_dict.keys():\n",
    "#                 wrd.pos = pos_dict[word.pos]\n",
    "#             temp=(wrd.text,wrd.pos)\n",
    "            \n",
    "#             s.append(temp)\n",
    "        \n",
    "\n",
    "#         return(s)   \n",
    "\n",
    "\n",
    "# stand_sent=[]\n",
    "# for sentence in tagged_sent:\n",
    "#     s=\"\"\n",
    "#     leng=len(sentence)\n",
    "#     print(leng)\n",
    "#     for w in range(leng):\n",
    "#         s = s+\" {}\".format(sentence[w][0])\n",
    "#     s = s[1:]\n",
    "#     doc = nlp(s)\n",
    "#     stanfrdnlp = extract_pos(doc)\n",
    "#     stand_sent.append(stanfrdnlp)\n",
    "    \n",
    "\n",
    "\n",
    "# len(tagged_sent)\n",
    "# for s_ind in range(len(tagged_sent)):\n",
    "#     for w_ind in range(len(tagged_sent[s_ind])):\n",
    "#         if(tagged_sent[s_ind][w_ind][1] == 'RB' or tagged_sent[s_ind][w_ind][1] == 'JJ'):\n",
    "#             try:\n",
    "#                 tagged_sent[s_ind][w_ind] = list(tagged_sent[s_ind][w_ind])\n",
    "#                 print(\"s\",s_ind,\"w\",w_ind, tagged_sent[s_ind],tagged_sent[s_ind][w_ind],tagged_sent[s_ind][w_ind][1])\n",
    "#                 tagged_sent[s_ind][w_ind][1] = stand_sent[s_ind][w_ind][1]\n",
    "#                 tagged_sent[s_ind][w_ind]=tuple(tagged_sent[s_ind][w_ind])\n",
    "#             except: \n",
    "#                 continue\n",
    "\n",
    "\n",
    "\n",
    "# ## ACCURACY CALCULATION\n",
    "# list_manual=[word[1] for sent in tagged_sent for word in sent]\n",
    "# list_tagger=[word[1] for sent in validate_data for word in sent]\n",
    "# matrix=confusion_matrix(list_tagger,list_manual,labels=list(set(list_tagger)))\n",
    "# df=pd.DataFrame(matrix,index=list(set(list_tagger)),columns=list(set(list_tagger)))\n",
    "# summ=0\n",
    "# acuuracy=0\n",
    "# for tag in list(set(list_tagger)):\n",
    "#     summ+=df[tag][tag]\n",
    "#     #print(tag,summ)\n",
    "\n",
    "\n",
    "# total = len([word for sent in tagged_sent for word in sent])\n",
    "# accuracy=summ/total   \n",
    "\n",
    "\n",
    "# print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cffe79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd8def1",
   "metadata": {},
   "source": [
    "# **7. Only Single Tag**\n",
    "*Assigning the same tag to the word which are only tagged with a single type of category thoroughly in the training_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jr_words = [word[0] for sent in train_data for word in sent]\n",
    "# jr_words = list(set(jr_words))\n",
    "# word_dict={}\n",
    "# for jr in jr_words:\n",
    "#     temp=[]\n",
    "#     for sent in tagged_sent:\n",
    "#         for word in sent:\n",
    "#             if(jr == word[0]):\n",
    "#                 temp.append(word[1])\n",
    "#     if(len(list(set(temp))) == 1):\n",
    "#         if('RB' in temp or 'JJ' in temp):\n",
    "#             word_dict[jr]=list(set(temp))\n",
    "    \n",
    "    \n",
    "\n",
    "# for sent in tagged_sent:\n",
    "#     for word in sent:\n",
    "#         if(word[0] in word_dict.keys()):\n",
    "#             word = list(word)\n",
    "#             w=word[0]\n",
    "#             word[1] = word_dict[w]\n",
    "#             word = tuple(word)\n",
    "\n",
    "\n",
    "\n",
    "# ## ACCURACY CALCULATION\n",
    "# list_manual=[word[1] for sent in tagged_sent for word in sent]\n",
    "# list_tagger=[word[1] for sent in test_data for word in sent]\n",
    "# matrix=confusion_matrix(list_tagger,list_manual,labels=list(set(list_tagger)))\n",
    "# df=pd.DataFrame(matrix,index=list(set(list_tagger)),columns=list(set(list_tagger)))\n",
    "# summ=0\n",
    "# acuuracy=0\n",
    "# for tag in list(set(list_tagger)):\n",
    "#     summ+=df[tag][tag]\n",
    "\n",
    "# total = len([word for sent in tagged_sent for word in sent])\n",
    "# accuracy=summ/total   \n",
    "\n",
    "\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45d16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ab8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
